title: 拥有自己的图书小金库
toc: true
date: 2016-01-09 14:15:46
tags:
	- python
	- 爬虫
	- django
---
> 在找matering elasticsearch second edition这本书的时候，在许多可以免费下电子书的网站都没有找到，所以就买了[Geekbook](https://www.geekbooks.me/category)一个月的会员，为了不浪费充会员的钱，决定撸个脚本把全站的书都下下来,并用django搭个管理后台。和[洪菊](https://github.com/KevinOfNeu)，[卢神](https://github.com/stephenluu)利用空闲时间忙活了快两周，终于有了自己的图书小金库，5000+的优质英文原版电子书，哇咔咔咔咔，这辈子的书都有了。
geekbook千万不要怪我们啊，谁让你不封我们的~~。
**项目地址：[github](https://github.com/giraffe0813/GeekBook)**

![小黄人](/images/xiaohuangren.gif)
<!-- more -->

### 遇到的坑
记录下遇到的坑 不然就忘了😂
#### 只带cookie 无法下载
本来以为只要带着登录之后的cookie请求下载的地址(eg:https://www.geekbooks.me/books/56/48/c1955c13518f994167b11f7b7279/amazon_ec2_cookbook.pdf) 就可以下载了，不过发现下下来的并不是书，而是网站上书的详情页的html。受到博客[Course抓站小结](http://www.jianshu.com/p/c3dbf8294c33) 的启发，又研究了一下请求的header。
![header](/images/header.jpg)
发现除了要带cookie之外，还要带有user_agent和Referer，refer是表示从哪个页面访问当前链接。
所以修改了下代码，在请求头中加入user-agent和referer之后，问题就解决了,部分代码如下
```python
 cookie = cookielib.MozillaCookieJar()
 # get cookie from file
 cookie.load('../data/cookie4geek.data', ignore_discard=True, ignore_expires=True)
 handler = urllib2.HTTPCookieProcessor(cookie)
 opener = urllib2.build_opener(handler)
 # add header
 opener.addheaders = [('User-agent', 'Mozilla/5.0'), ("Referer", url)]
```
#### 下载文件的时候不显示进度
可以下载文件之后，希望可以在下载文件的时候显示下载进度，让我们知道他在工作。。。。直接在stackoverflow上抄了段代码，这个问题也愉快的解决了。
```python
        u = opener.open("https://www.geekbooks.me" + url)
        print "Preparing to download..."
        # f with directory
        if os.path.exists(conf_books_dir + category + "/" + file_name) and detect_book(
                (conf_books_dir + category + "/" + file_name)):
            continue
        f = open(conf_books_dir + category + "/" + file_name, 'wb')
        meta = u.info()
        file_size = int(meta.getheaders("Content-Length")[0])
        print "Downloading: %s Bytes: %s" % (file_name, file_size)
        file_size_dl = 0
        block_sz = 8192
        while True:
            buffer = u.read(block_sz)
            if not buffer:
                break
            file_size_dl += len(buffer)
            f.write(buffer)
            status = r"%10d  [%3.2f%%]" % (file_size_dl, file_size_dl * 100. / file_size)
            status = status + chr(8) * (len(status) + 1)
            print status,
        f.close()
```
#### 一本一本下载速度太慢
这个只能果断上多线程了。。。
```python
def download_work():
    f = open("../data/detailurl.txt", "r")
    books = []
    destDir = ""
    tmp = ""
    for line in f:
        if not (line.strip()).startswith("/"):
            tmp += "/" + line.strip()
            destDir = tmp
        else:
            # desDir
            book = Book(destDir, line.strip())
            books.append(book)
            tmp = ""
    pool = threadpool.ThreadPool(conf_thread_count)
    reqs = threadpool.makeRequests(lambda book: book.download(), books)
    [pool.putRequest(req) for req in reqs]
    pool.wait()
```
#### 无法展示下载书的具体信息
除了将书下载下来，还想将书的一些基本信息保存下来，比如:作者，简洁，出版年份，封面，标签等等。。最好可以根据作者，题目进行搜索。本来想自己写个网站出来，但是又没有时间。还好之前学叔推荐过django，用django搭个管理后台简直不要太方便好么，配置nginx，supervisor的时间都比写代码的时间长👅，还可以很方便定义想搜索的字段和想展示的字段。样式是丑了一丢丢，但自己用也无所谓。
### 成品
![admin](/images/admin.jpg)

### Summary
依靠空闲时间可以做点想做的事，也是蛮好的~~。用python写脚本真的很方便，基本不用自己造轮子，用它自带的模块就可以完成了。用django搭管理后台也是快的不要不要的。一周多的时间换4000+的书很值啊，但是。。。。服务器+存储平均一天的成本就要10块。。。是不是得想个法子，看能不能用这些电子书挣点钱啊。。。。想到这。。。突然没那么开心了。。。
![lingluan](/images/lingluan.jpg)







